{
  "hash": "61169b0be505f2b5d5a9dd60ecd28e8a",
  "result": {
    "markdown": "---\ntitle: \"{{< var title >}}: {{< var version >}}\"\nsubtitle: \"Eye-tracking data clean-up\"\nauthor: \"{{< var author.name >}}\"\ndate: \"2022-10-03\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(here::here(\"scripts\", \"00_libraries.R\"))\nsource(here::here(\"scripts\", \"01_helpers.R\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data and fix accent mark chars\nstress_50_temp <- read_tsv(here(\"data\", \"raw\", \"stress_extract_50.txt\")) %>% \n  mutate(target = str_replace(target, \"Ã³\", \"o\"))\n```\n:::\n\n\nImportant variable info: \n\n- AVERAGE_IA_1_SAMPLE_COUNT: looks at target\n- AVERAGE_IA_2_SAMPLE_COUNT: looks at distractor\n- AVERAGE_IA_0_SAMPLE_COUNT: looks elsewhere\n\nHow much data do we lose by selecting only accurate trials: 0.0055517.\n\n\nNow we take a look at item lengths and check some landmarks. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nitemlength <- stress_50_temp %>%\n  select(., target, t01, t04, t05, t06) %>%\n  distinct() %>% \n  mutate(\n    syll1_dur = t04 - t01, \n    syll2_dur = t06 - t04, \n    C3_dur    = t05 - t04, \n    item_dur  = t06 - t01, \n    derived_dur = syll1_dur + syll2_dur, \n    check = item_dur == derived_dur\n  )\n\nknitr::kable(itemlength %>% head, format = \"pandoc\")\n```\n\n::: {.cell-output-display}\ntarget      t01    t04    t05    t06   syll1_dur   syll2_dur   C3_dur   item_dur   derived_dur  check \n--------  -----  -----  -----  -----  ----------  ----------  -------  ---------  ------------  ------\nplancho     742   1091   1167   1262         349         171       76        520           520  TRUE  \nfirma       930   1185   1274   1336         255         151       89        406           406  TRUE  \nbusco       907   1207   1230   1332         300         125       23        425           425  TRUE  \ncanta       712   1081   1092   1177         369          96       11        465           465  TRUE  \ncompra      878   1207   1262   1336         329         129       55        458           458  TRUE  \ngasto      1025   1320   1334   1457         295         137       14        432           432  TRUE\n:::\n:::\n\n\n\nOk, let's tidy and center the time course.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstress_50_hold <- stress_50_temp %>%\n  \n  # create id variable\n  rename(id = RECORDING_SESSION_LABEL) %>% \n  \n  # create group variable\n  separate(id, into = c(\"group\", \"trash\"), sep = 3, remove = F) %>%\n  \n  #select and rename variables of interest\n  dplyr::select(id, trial = TRIAL_INDEX, bin = BIN_INDEX,\n    target_count = AVERAGE_IA_1_SAMPLE_COUNT, \n    target_prop = `AVERAGE_IA_1_SAMPLE_COUNT_%`,\n    distractor_count = AVERAGE_IA_2_SAMPLE_COUNT, \n    distractor_prop = `AVERAGE_IA_2_SAMPLE_COUNT_%`, \n    elsewhere_count = AVERAGE_IA_0_SAMPLE_COUNT, \n    elsewhere_prop = AVERAGE_IA_0_SAMPLE_COUNT, \n    accuracy = ACCURACY, rt = RT, block, stress = cond, \n    offset_prev_word = t01, \n    onset_v1 = t02, \n    onset_c2 = t03, \n    onset_c3 = t04, \n    onset_v2 = t05, \n    offset_target = t06, \n    end_sentence = t07, target, version, group) %>%\n\n  # remove incorrect\n  filter(., accuracy == 1) %>%\n\n  # Create eLog variable and respective wts\n  mutate(eLog = log((target_count + 0.5) / (50 - target_count + 0.5)),\n         wts = 1 / (target_count + 0.5) + 1 / (50 - target_count + 0.5)) %>%\n    \n  # Select necessary columns\n  dplyr::select(id, group, target, stress, bin,\n    target_count, target_prop, eLog, wts, onset_c3, \n    elsewhere_count, elsewhere_prop, distractor_count, distractor_prop) %>% \n\n  # Get suffix onset label and center at 0 for each\n  # participant for each item\n  # change 'onset_c3' in previous `select` call to center around diff. landmark\n  mutate(lm_bin = (onset_c3 / 50) %>% ceiling(.),\n         t_onset = if_else(bin == lm_bin, TRUE, FALSE)) %>% \n  group_by(id, target) %>%\n  mutate(time_zero = center_timecourse(bin, t_onset, event = \"TRUE\")) %>% \n  ungroup()\n```\n:::\n\n\n\n\nChecks and fixes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove unwanted groups\nstress_50 <- stress_50_hold %>% \n  filter(!(group %in% c(\"ams\", \"ims\"))) %>% \n  mutate(id = fct_recode(id, ihs02 = \"ih02\"), \n    group = fct_recode(group, ihs = \"ih0\"), \n    group = case_when(\n      group %in% c(\"ihs\", \"ahs\") ~ \"HL\", \n      group %in% c(\"ies\", \"aes\") ~ \"L2\", \n      TRUE ~ \"Monolingual\"), \n    stress = if_else(stress == 2, \"Oxytone\", \"Paroxytone\")\n    ) \n\n# N x group\nstress_50 %>% \n  group_by(group) %>% \n  summarize(n = n_distinct(id))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  group           n\n  <chr>       <int>\n1 HL             49\n2 L2             65\n3 Monolingual    30\n```\n:::\n\n```{.r .cell-code}\n# Plot check\nstress_50 %>% \n  filter(time_zero > -6 & time_zero < 11) %>% \n  mutate(stress = fct_relevel(stress, \"Paroxytone\")) %>% \n  ggplot(., aes(x = time_zero, y = target_prop)) + \n    facet_grid(. ~ stress) + \n    geom_hline(yintercept = 0.5, lty = 3) + \n    geom_vline(xintercept = 0 + 4, lty = 3) + \n    stat_summary(aes(fill = group), fun.data = mean_cl_boot, geom = \"ribbon\", \n      alpha = 0.5) + \n    stat_summary(aes(color = group), fun = mean, geom = \"line\") + \n    scale_color_viridis_d(name = NULL, end = 0.8) + \n    scale_fill_viridis_d(name = NULL, end = 0.9) + \n    coord_cartesian(xlim = c(NA, 10), ylim = c(-0.05, 1.05), expand = F) + \n    scale_x_continuous(breaks = seq(-4, 12, 4), labels = seq(-200, 600, 200)) + \n    labs(y = \"P(target fixation)\", x = \"Time course\") + \n    theme(legend.position = c(0.1, 0.86)) + \n    NULL\n```\n\n::: {.cell-output-display}\n![](../figs/checks-fixes-1.png){width=100%}\n:::\n:::\n\n\nLooks ok. \nNow we add proficiency and use data for the bilingual groups, then we save the complete data frame and split it by ID so it will fit on github.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load prof/use data\nprof_use <- read_csv(here(\"data\", \"tidy\", \"prof_use.csv\")) %>% \n  select(id, prof, use, remove) %>% \n  filter(remove == 0) %>% \n  mutate(prof_z = (prof - mean(prof)) / sd(prof), \n    use_z = (use - mean(use)) / sd(use))\n\n# Get vector of IDs to keep\nkeep_ids <- c(\n  prof_use$id %>% unique, \n  filter(stress_50, group == \"Monolingual\") %>% pull(id) %>% \n    unique %>% as.character\n)\n\n# Check n per group to see if filter is correct\n# Should be 42 HL and 50 L2\nstress_50 %>% \n  filter(id %in% keep_ids) %>% \n  group_by(group) %>% \n  summarize(n = n_distinct(id))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  group           n\n  <chr>       <int>\n1 HL             42\n2 L2             50\n3 Monolingual    30\n```\n:::\n\n```{.r .cell-code}\n# Join data frames and save\nif(F) {\nleft_join(stress_50 %>% filter(id %in% keep_ids), prof_use, by = \"id\") %>%\n  select(id:group, prof:use, prof_z:use_z, target:time_zero) %>% \n  write_csv(here(\"data\", \"tidy\", \"stress_50ms.csv\")) %>% \n  group_by(id) %>% \n  group_walk(~ write_csv(.x, \n    here(\"data\", \"tidy\", \"by_id_50ms\", \n      paste0(.y$id, \"_stress_50ms.csv\"))))\n}\n```\n:::\n",
    "supporting": [
      "03_tidy_eyetracking_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}